{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_text\n",
    "from sklearn import preprocessing\n",
    "from id3 import Id3Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': '/Users/Tasya/anaconda3/lib/python3.6/site-packages/sklearn/datasets/data/iris.csv'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read iris dataset\n",
    "data_iris = datasets.load_iris()\n",
    "data_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['D1', 'Sunny', 'Hot', 'High', 'Weak', 'No'],\n",
       " ['D2', 'Sunny', 'Hot', 'High', 'Strong', 'No'],\n",
       " ['D3', 'Overcast', 'Hot', 'High', 'Weak', 'Yes'],\n",
       " ['D4', 'Rain', 'Mild', 'High', 'Weak', 'Yes'],\n",
       " ['D5', 'Rain', 'Cool', 'Normal', 'Weak', 'Yes'],\n",
       " ['D6', 'Rain', 'Cool', 'Normal', 'Strong', 'No'],\n",
       " ['D7', 'Overcast', 'Cool', 'Normal', 'Strong', 'Yes'],\n",
       " ['D8', 'Sunny', 'Mild', 'High', 'Weak', 'No'],\n",
       " ['D9', 'Sunny', 'Cool', 'Normal', 'Weak', 'Yes'],\n",
       " ['D10', 'Rain', 'Mild', 'Normal', 'Weak', 'Yes'],\n",
       " ['D11', 'Sunny', 'Mild', 'Normal', 'Strong', 'Yes'],\n",
       " ['D12', 'Overcast', 'Mild', 'High', 'Strong', 'Yes'],\n",
       " ['D13', 'Overcast', 'Hot', 'Normal', 'Weak', 'Yes'],\n",
       " ['D14', 'Rain', 'Mild', 'High', 'Strong', 'No']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read play-tennis dataset\n",
    "data_play_tennis = pd.read_csv('play-tennis.csv')\n",
    "data_play_tennis = data_play_tennis.values.tolist()\n",
    "data_play_tennis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['No'],\n",
       " ['No'],\n",
       " ['Yes'],\n",
       " ['Yes'],\n",
       " ['Yes'],\n",
       " ['No'],\n",
       " ['Yes'],\n",
       " ['No'],\n",
       " ['Yes'],\n",
       " ['Yes'],\n",
       " ['Yes'],\n",
       " ['Yes'],\n",
       " ['Yes'],\n",
       " ['No']]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play-tennis target\n",
    "data_play_tennis_target = []\n",
    "for x in data_play_tennis:\n",
    "    data_play_tennis_target.append(x[-1:])\n",
    "data_play_tennis_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Sunny', 'Hot', 'High', 'Weak'],\n",
       " ['Sunny', 'Hot', 'High', 'Strong'],\n",
       " ['Overcast', 'Hot', 'High', 'Weak'],\n",
       " ['Rain', 'Mild', 'High', 'Weak'],\n",
       " ['Rain', 'Cool', 'Normal', 'Weak'],\n",
       " ['Rain', 'Cool', 'Normal', 'Strong'],\n",
       " ['Overcast', 'Cool', 'Normal', 'Strong'],\n",
       " ['Sunny', 'Mild', 'High', 'Weak'],\n",
       " ['Sunny', 'Cool', 'Normal', 'Weak'],\n",
       " ['Rain', 'Mild', 'Normal', 'Weak'],\n",
       " ['Sunny', 'Mild', 'Normal', 'Strong'],\n",
       " ['Overcast', 'Mild', 'High', 'Strong'],\n",
       " ['Overcast', 'Hot', 'Normal', 'Weak'],\n",
       " ['Rain', 'Mild', 'High', 'Strong']]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play-tennis data\n",
    "data_play_tennis_data = []\n",
    "for x in data_play_tennis:\n",
    "    data_play_tennis_data.append(x[:-1][1:])\n",
    "data_play_tennis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- petal length (cm) <= 2.45\n",
      "|   |--- class: 0\n",
      "|--- petal length (cm) >  2.45\n",
      "|   |--- petal width (cm) <= 1.75\n",
      "|   |   |--- petal length (cm) <= 4.95\n",
      "|   |   |   |--- petal width (cm) <= 1.65\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- petal width (cm) >  1.65\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |--- petal length (cm) >  4.95\n",
      "|   |   |   |--- petal width (cm) <= 1.55\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- petal width (cm) >  1.55\n",
      "|   |   |   |   |--- sepal length (cm) <= 6.95\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- sepal length (cm) >  6.95\n",
      "|   |   |   |   |   |--- class: 2\n",
      "|   |--- petal width (cm) >  1.75\n",
      "|   |   |--- petal length (cm) <= 4.85\n",
      "|   |   |   |--- sepal width (cm) <= 3.10\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- sepal width (cm) >  3.10\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- petal length (cm) >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier iris\n",
    "iris_tree = tree.DecisionTreeClassifier()\n",
    "iris_tree = iris_tree.fit(data_iris.data, data_iris.target)\n",
    "iris_tree_text = export_text(iris_tree, feature_names=data_iris['feature_names'])\n",
    "print(iris_tree_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tasya/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['No', 'Yes']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess play-tennis target\n",
    "tennis = preprocessing.LabelEncoder()\n",
    "tennis.fit(data_play_tennis_target)\n",
    "list(tennis.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tasya/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode play-tennis target\n",
    "data_play_tennis_target_labeled = tennis.transform(data_play_tennis_target)\n",
    "data_play_tennis_target_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1, 0, 1],\n",
       " [2, 1, 0, 0],\n",
       " [0, 1, 0, 1],\n",
       " [1, 2, 0, 1],\n",
       " [1, 0, 1, 1],\n",
       " [1, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [2, 2, 0, 1],\n",
       " [2, 0, 1, 1],\n",
       " [1, 2, 1, 1],\n",
       " [2, 2, 1, 0],\n",
       " [0, 2, 0, 0],\n",
       " [0, 1, 1, 1],\n",
       " [1, 2, 0, 0]]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess play-tennis data\n",
    "data_play_tennis_data_transpose = map(list, zip(*data_play_tennis_data))\n",
    "data_play_tennis_data_labeled = []\n",
    "for x in data_play_tennis_data_transpose:\n",
    "    tennis.fit(x)\n",
    "    data_play_tennis_data_labeled.append(tennis.transform(x)) \n",
    "data_play_tennis_data_labeled = list(map(list, zip(*data_play_tennis_data_labeled)))\n",
    "data_play_tennis_data_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- outlook <= 0.50\n",
      "|   |--- class: 1\n",
      "|--- outlook >  0.50\n",
      "|   |--- humidity <= 0.50\n",
      "|   |   |--- outlook <= 1.50\n",
      "|   |   |   |--- wind <= 0.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- wind >  0.50\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- outlook >  1.50\n",
      "|   |   |   |--- class: 0\n",
      "|   |--- humidity >  0.50\n",
      "|   |   |--- wind <= 0.50\n",
      "|   |   |   |--- temp <= 1.00\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- temp >  1.00\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- wind >  0.50\n",
      "|   |   |   |--- class: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier play-tennis\n",
    "play_tennis_tree = tree.DecisionTreeClassifier()\n",
    "play_tennis_tree = play_tennis_tree.fit(data_play_tennis_data_labeled, data_play_tennis_target_labeled)\n",
    "play_tennis_tree_text = export_text(play_tennis_tree, feature_names = ['outlook', 'temp', 'humidity', 'wind'])\n",
    "print(play_tennis_tree_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "petal length (cm) <=2.45: 0 (50) \n",
      "petal length (cm) >2.45\n",
      "|   petal width (cm) <=1.75\n",
      "|   |   sepal length (cm) <=7.10\n",
      "|   |   |   sepal width (cm) <=2.85: 1 (27/4) \n",
      "|   |   |   sepal width (cm) >2.85: 1 (22) \n",
      "|   |   sepal length (cm) >7.10: 2 (1) \n",
      "|   petal width (cm) >1.75\n",
      "|   |   sepal length (cm) <=5.95\n",
      "|   |   |   sepal width (cm) <=3.10: 2 (6) \n",
      "|   |   |   sepal width (cm) >3.10: 1 (1) \n",
      "|   |   sepal length (cm) >5.95: 2 (39) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from id3 import export_text\n",
    "# Id3Estimator iris\n",
    "estimator_iris = Id3Estimator()\n",
    "estimator_iris = estimator_iris.fit(data_iris.data, data_iris.target)\n",
    "estimator_iris_text = export_text(estimator_iris.tree_, feature_names=data_iris['feature_names'])\n",
    "print(estimator_iris_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "outlook <=0.50: 1 (4) \n",
      "outlook >0.50\n",
      "|   humidity <=0.50\n",
      "|   |   temp <=1.50: 0 (2) \n",
      "|   |   temp >1.50\n",
      "|   |   |   wind <=0.50: 0 (1) \n",
      "|   |   |   wind >0.50: 0 (1/1) \n",
      "|   humidity >0.50\n",
      "|   |   wind <=0.50\n",
      "|   |   |   temp <=1.00: 0 (1) \n",
      "|   |   |   temp >1.00: 1 (1) \n",
      "|   |   wind >0.50: 1 (3) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Id3Estimator play-tennis\n",
    "estimator_play_tennis = Id3Estimator()\n",
    "estimator_play_tennis = estimator_play_tennis.fit(data_play_tennis_data_labeled, data_play_tennis_target_labeled)\n",
    "r = export_text(estimator_play_tennis.tree_, feature_names = [\"outlook\",\"temp\",\"humidity\",\"wind\"])\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Algoritma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penentuan atribut terbaik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada library DecisionTreeClassifier menentukan atribut terbaik dengan salah satu algoritma decision tree yaitu CART (Classification and Regression Tree) yang memanfaatkan Gini Index. Untuk atribut yang bernilai diskrit, splitting atribut akan dipilih berdasarkan subset dengan Gini Index terkecil (minimum).\n",
    "\n",
    "Algoritma ID3 pada buku menentukan atribut terbaik dengan konsep entropy untuk menilai seberapa informatif suatu node. Kemudian, pemilihan atribut dilakukan dengan nilai Information Gain berdasarkan nilai Information Gain terbesar.\n",
    "\n",
    "Pada modul ID3Estimator sama dengan algoritma pada buku karena modul ini dibangun menggunakan algoritma ID3\n",
    "\n",
    "Persamaan: Melakukan pemilihan atribut berdasarkan pilihan terbaik, sesuai dengan attribute selection measures masing masing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penanganan label dari cabang setiap nilai atribut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritma ID3 pada buku menetukan label berdasarkan nilai data dari example. Jika semua example bernilai positif akan dibentuk single-node tree dari root dengan label \"+\" (positif). Begitu juga jika semua example bernilai negatif akan dibentuk single-node tree dari root dengan label \"-\" (negatif). Begitu seterusnya hingga ditemukan example kosong atau atribut sudah kosong.\n",
    "\n",
    "Pada library DecisionTreeClassifier yang menggunakan algoritma CART, pemberian label dilakukan mengikuti aturan jumlah terbanyak, karena proses ini akan memilih label dengan peluang terbesar. Begitu juga untuk example dengan data yang homogen, proses splitting akan dihentikan dan diberi label berdasarkan nilai terbanyak yaitu nilai satu satunya yang ada pada example (karena homogen).\n",
    "\n",
    "Pada modul ID3Estimator penanganan label sama seperti algoritma ID3 pada buku.\n",
    "\n",
    "Persamaan: Penanganan label pada data yang homogen, menghentikan proses splitting dan membentuk single-node dengan nilai yang sesuai dengan nilai pada data homogen tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penentuan label jika examples kosong di cabang tersebut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada library DecisionTreeClassifier menentukan label jika example kosong sama dengan penanganan label seperti biasanya, seperti yang telah dijabarkan sebelumnya pada penanganan label dari cabang setiap nilai atribut. DecisionTreeClassifier akan melakukan proses pelabelan yang sama hingga node terakhir.\n",
    "\n",
    "Algoritma ID3 pada buku memberi label jika ditemukan example kosong dengan membentuk leaf node pada cabang tersebut dengan label nilai modus (paling sering muncul) dari target atribut pada example.\n",
    "\n",
    "Pada modul ID3Estimator penanganan label sama seperti algoritma ID3 pada buku yaitu dengan melakukan pelabelan sesuai data terbanyak dari atribut pada example.\n",
    "\n",
    "Persamaan: Pada penanganan label pada example kosong, pada dasarnya ketiganya melakukan proses yang sama yaitu memeberi label sesuai dengan data terbanyak pada atribut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penanganan atribut kontinu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library DecisionTreeClassifier mempartisi nilai-nilai pada atribut kontinu tersebut ke interval-interval untuk menentukan kelas yang cocok untuk tiap instans, sehingga tiap node pada decision tree akan menghitung nilai dari atribut suatu instans dan mencocokkan nilai tersebut ke interval yang sudah ditentukan. Misalnya, untuk dataset jenis kelamin, dan terdapat atribut tinggi badan, maka dapat diaproksimasi interval tinggi < 160 cm akan menjadi kelas 'wanita', dan tinggi >= 160 cm masuk ke kelas 'pria'.\n",
    "\n",
    "Id3Estimator men'diskrit'kan nilai-nilai kontinu untuk memudahkan klasifikasi. Untuk tiap nilai pada tiap atribut kontinu, akan dihitung suatu poin yang dapat membagi (split) range nilai atribut ke grup yang paling homogen pada tiap level pembagian, dengan cara menghitung information gain dari tiap kemungkinan splitting dan memilih splitting dengan information gain terbesar.\n",
    "\n",
    "Untuk algoritma pada buku, mirip dengan Id3Estimator, juga ditentukan suatu poin splitting yang memiliki information gain terbesar, sehingga dapat mengkategorikan nilai-nilai kontinu ke grup yang sehomogen mungkin.\n",
    "\n",
    "Persamaan: nilai-nilai atribut dibagi dengan satu atau beberapa interval untuk proses kategorisasi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penanganan atribut dengan missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada DecisionTreeClassifier, missing value diabaikan hanya saat evaluasi (penentuan titik splitting), sedangkan saat klasifikasi, instans dengan missing value tetap dikategorikan. Instans yang berisi missing values biasanya dikategorikan ke target dengan jumlah instans terbanyak yang sudah dikategorikan (modus), ataupun dikategorikan ke kelas lain dengan perhitungan yang sudah disesuaikan.\n",
    "\n",
    "Id3Estimator secara default mengabaikan instans dengan missing values, dan dapat berakibat pada tidak optimalnya hasil yang didapatkan seandainya instans yang diabaikan memegang peran penting (memiliki bobot yang besar) pada pembentukan pohon.\n",
    "\n",
    "Algoritma pada buku melabel instans dengan missing values dengan jumlah nilai label terbanyak di data latih (modus).\n",
    "\n",
    "Persamaan: Tidak ada kesamaan ketiga pendekatan dalam penanganan missing values. Namun, untuk menghindari pengurangan keakurasian hasil karena missing values, biasanya dilakukan imputasi data pada dataset, yaitu menghitung atau memperkirakan nilai yang hilang tersebut. Nilai yang biasanya dipakai adalah median (nilai tengah), mean (rata-rata) ataupun modus (nilai yang paling sering muncul)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning dan paraeter confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning pada DecisionTreeClassifier dilakukan dengan mengubah nilai parameter `min_samples_leaf` dan `max_depth` yang mengatur jumlah sampel minimum yang diperlukan untuk melakukan splitting dan kedalaman dari pohon. Terdapat pula opsi lain untuk mengatur ukuran dari pohon yang dihasilkan, yaitu parameter `cpp_alpha`. Semakin besar nilai `cpp_alpha`, semakin banyak node yang akan di-prune. Penentuan nilai `cpp_alpha` yang optimal sangat penting, karena node yang di-prune diharapkan tidak terlalu banyak sehingga nilai parameter confidence berkurang, namun juga tidak terlalu sedikit agar tidak menyebabkan overfitting. Telah disediakan fungsi `DecisionTreeClassifier.cost_complexity_pruning_path` dari scikit-learn yang mengembalikan nilai `cpp_alpha` yang efektif, dimana prioritas pruning yang diterapkan adalah terhadap node dengan link terlemah.\n",
    "\n",
    "Secara default, Id3Estimator akan terus membangun pohon sampai didapatkan tingkat parameter confidence setinggi mungkin dan error pada data latih mencapai 0%, sehingga pruning harus dilakukan agar menghindari overfitting. Proses pruning yang dilakukan disebut juga sebagai Reduced-Error Pruning, dimulai dengan menjadikan tiap node bukan daun menjadi daun, lalu node tersebut di-assign nilai yang paling umum dari anak-anak node tersebut. Bila error yang dihasilkan pohon baru ini pada data validasi tidak lebih buruk dari error yang dihasilkan pohon original, maka pohon baru ini akan menggantikan pohon yang lama. Proses ini dilakukan terus sampai tingkat error yang dihasilkan meningkat drastis, yang menyebabkan pengurangan tingkat akurasi pohon.\n",
    "\n",
    "Algoritma pada buku mirip dengan Id3Estimator, yaitu mengaplikasikan pruning dengan membuang node yang tidak 'berbahaya', yaitu node yang dapat menurunkan tingkat akurasi dan parameter confidence pohon secara signifikan.\n",
    "\n",
    "Persamaan: ketiga pendekatan mengatur ukuran pohon yang dihasilkan dengan membuang node yang dianggap tidak mengganggu dan mengubah tingkat akurasi pohon, sehingga pohon yang dihasilkan tidak terlalu spesifik untuk data latih saja."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
